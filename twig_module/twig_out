Hello, I am Twig. I do my job!
Loading job details...success
Doing my job!
=========================NAS Starting=========================
{'run_id': 'twig_run', 'n_iterations': 1, 'host_addr': '127.0.0.1', 'min_budget': 10, 'max_budget': 300, 'idgl_config_file': '/workspace/GNN_module/src/config/cora/idgl_anchor.yml', 'idgl_working_dir': '/workspace/GNN_module/src/', 'idgl_params': [None], 'hyperparameters': {'learning_rate': {'min': 0, 'max': 10, 'type': 'uniform_float'}, 'weight_decay': {'min': 0, 'max': 1, 'type': 'uniform_float'}, 'lr_reduce_factor': {'min': 0, 'max': 1, 'type': 'uniform_float'}, 'lr_patience': {'min': 1, 'max': 10, 'type': 'uniform_integer'}, 'grad_accumulated_steps': {'min': 1, 'max': 10, 'type': 'uniform_integer'}, 'dropout': {'min': 0, 'max': 1, 'type': 'uniform_float'}, 'feat_adj_dropout': {'min': 0, 'max': 1, 'type': 'uniform_float'}, 'gl_dropout': {'min': 0, 'max': 1, 'type': 'uniform_float'}, 'num_anchors': {'min': 1, 'max': 10000, 'type': 'uniform_integer'}, 'hidden_size': {'min': 1, 'max': 200, 'type': 'uniform_integer'}, 'graph_skip_conn': {'min': 0, 'max': 1, 'type': 'uniform_float'}, 'update_adj_ratio': {'min': 0, 'max': 1, 'type': 'uniform_float'}, 'smoothness_ratio': {'min': 0, 'max': 1, 'type': 'uniform_float'}, 'degree_ratio': {'min': 0, 'max': 1, 'type': 'uniform_float'}, 'sparsity_ratio': {'min': 0, 'max': 1, 'type': 'uniform_float'}, 'graph_learn_hidden_size': {'min': 1, 'max': 200, 'type': 'uniform_integer'}, 'graph_learn_num_pers': {'min': 1, 'max': 20, 'type': 'uniform_integer'}, 'graph_hops': {'min': 1, 'max': 10, 'type': 'uniform_integer'}}}
{'min': 0, 'max': 10, 'type': 'uniform_float'}
{'min': 0, 'max': 1, 'type': 'uniform_float'}
{'min': 0, 'max': 1, 'type': 'uniform_float'}
{'min': 1, 'max': 10, 'type': 'uniform_integer'}
{'min': 1, 'max': 10, 'type': 'uniform_integer'}
{'min': 0, 'max': 1, 'type': 'uniform_float'}
{'min': 0, 'max': 1, 'type': 'uniform_float'}
{'min': 0, 'max': 1, 'type': 'uniform_float'}
{'min': 1, 'max': 10000, 'type': 'uniform_integer'}
{'min': 1, 'max': 200, 'type': 'uniform_integer'}
{'min': 0, 'max': 1, 'type': 'uniform_float'}
{'min': 0, 'max': 1, 'type': 'uniform_float'}
{'min': 0, 'max': 1, 'type': 'uniform_float'}
{'min': 0, 'max': 1, 'type': 'uniform_float'}
{'min': 0, 'max': 1, 'type': 'uniform_float'}
{'min': 1, 'max': 200, 'type': 'uniform_integer'}
{'min': 1, 'max': 20, 'type': 'uniform_integer'}
{'min': 1, 'max': 10, 'type': 'uniform_integer'}
**************** MODEL CONFIGURATION ****************
bignn                    -->   False
cuda_id                  -->   0
data_dir                 -->   ../data/cora/
data_type                -->   network
dataset_name             -->   cora
degree_ratio             -->   0.41040175712854543
dropout                  -->   0.44752675186871493
eary_stop_metric         -->   nloss
eps_adj                  -->   8.5e-05
feat_adj_dropout         -->   0.8804724846259077
gat_alpha                -->   0.2
gat_nhead                -->   8
gl_dropout               -->   0.23370055838458492
grad_accumulated_steps   -->   5
grad_clipping            -->   None
graph_hops               -->   7
graph_include_self       -->   False
graph_learn              -->   True
graph_learn_epsilon      -->   0
graph_learn_hidden_size  -->   77
graph_learn_num_pers     -->   11
graph_learn_ratio        -->   0
graph_learn_regularization -->   True
graph_learn_topk         -->   None
graph_metric_type        -->   weighted_cosine
graph_module             -->   gcn
graph_skip_conn          -->   0.8299863961185526
graph_type               -->   dynamic
hidden_size              -->   95
learning_rate            -->   5.571114234695301
logging                  -->   True
lr_patience              -->   9
lr_reduce_factor         -->   0.1662758206574767
max_epochs               -->   11.11111111111111
max_iter                 -->   10
max_rl_ratio             -->   0.99
model_name               -->   GraphClf
no_cuda                  -->   False
num_anchors              -->   7511
optimizer                -->   adam
out_dir                  -->   /tmp/iteration_id_0.2999572130455165
out_predictions          -->   False
out_raw_learned_adj_path -->   cora_idgl_node_anchor_adj.npy
patience                 -->   100
pretrain_epoch           -->   0
pretrained               -->   None
print_every_epochs       -->   500
random_seed              -->   1234
rl_ratio                 -->   0
rl_ratio_power           -->   1
rl_reward_metric         -->   acc
rl_start_epoch           -->   1
rl_wmd_ratio             -->   0
save_params              -->   True
scalable_run             -->   True
seed                     -->   42
shuffle                  -->   True
smoothness_ratio         -->   0.30674949828406783
sparsity_ratio           -->   0.07679041375068263
task_type                -->   classification
update_adj_ratio         -->   0.2872929411910876
use_bert                 -->   False
verbose                  -->   20
weight_decay             -->   0.01653012441192203
**************** MODEL CONFIGURATION ****************
[ Using CUDA ]
[ Using ground-truth input graph ]
[ Running GraphClf model ]
[ Multi-perspective weighted_cosine AnchorGraphLearner: 11 ]
[ Graph Learner metric type: weighted_cosine ]
[ Multi-perspective weighted_cosine AnchorGraphLearner: 11 ]
[ Graph Learner metric type: weighted_cosine ]
[ Graph Learner ]
[ Graph Regularization]
encoder.graph_encoders.0.weight: torch.Size([1433, 95])
encoder.graph_encoders.1.weight: torch.Size([95, 95])
encoder.graph_encoders.2.weight: torch.Size([95, 95])
encoder.graph_encoders.3.weight: torch.Size([95, 95])
encoder.graph_encoders.4.weight: torch.Size([95, 95])
encoder.graph_encoders.5.weight: torch.Size([95, 95])
encoder.graph_encoders.6.weight: torch.Size([95, 7])
graph_learner.weight_tensor: torch.Size([11, 1433])
graph_learner2.weight_tensor: torch.Size([11, 95])
#Parameters = 198733

<> <> <> Starting Timer [Train] <> <> <>
**************** MODEL CONFIGURATION ****************
bignn                    -->   False
cuda_id                  -->   0
data_dir                 -->   ../data/cora/
data_type                -->   network
dataset_name             -->   cora
degree_ratio             -->   0.5821923794207193
device                   -->   cuda:0
dropout                  -->   0.932573785305208
eary_stop_metric         -->   nloss
eps_adj                  -->   8.5e-05
feat_adj_dropout         -->   0.7598433390879036
gat_alpha                -->   0.2
gat_nhead                -->   8
gl_dropout               -->   0.30655259677558966
grad_accumulated_steps   -->   10
grad_clipping            -->   None
graph_hops               -->   6
graph_include_self       -->   False
graph_learn              -->   True
graph_learn_epsilon      -->   0
graph_learn_hidden_size  -->   188
graph_learn_num_pers     -->   5
graph_learn_ratio        -->   0
graph_learn_regularization -->   True
graph_learn_topk         -->   None
graph_metric_type        -->   weighted_cosine
graph_module             -->   gcn
graph_skip_conn          -->   0.4551820390629502
graph_type               -->   dynamic
hidden_size              -->   72
learning_rate            -->   5.635423375770738
logging                  -->   True
lr_patience              -->   6
lr_reduce_factor         -->   0.05856790400471634
max_epochs               -->   11.11111111111111
max_iter                 -->   10
max_rl_ratio             -->   0.99
model_name               -->   GraphClf
no_cuda                  -->   False
num_anchors              -->   4308
num_class                -->   7
num_feat                 -->   1433
optimizer                -->   adam
out_dir                  -->   /tmp/iteration_id_0.6337327297325777
out_predictions          -->   False
out_raw_learned_adj_path -->   cora_idgl_node_anchor_adj.npy
patience                 -->   100
pretrain_epoch           -->   0
pretrained               -->   None
print_every_epochs       -->   500
random_seed              -->   1234
rl_ratio                 -->   0
rl_ratio_power           -->   1
rl_reward_metric         -->   acc
rl_start_epoch           -->   1
rl_wmd_ratio             -->   0